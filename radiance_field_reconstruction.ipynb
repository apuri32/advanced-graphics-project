{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d866343e-dc43-44e1-b61d-2ed1c7a82be0",
   "metadata": {},
   "source": [
    "Based on https://mitsuba.readthedocs.io/en/latest/src/inverse_rendering/radiance_field_reconstruction.html. To run this section of the tutorial, first run\n",
    "\n",
    "pip install mitsuba\n",
    "\n",
    "from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498ee94-459a-40c3-987c-eb4f40e3f9c7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We must import DrJit and Mitsuba and set a variant that supports automatic differentiation. ipywidgets are used to provide an interactive user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec80cc-d2a5-46da-83bd-cd66f8947a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "mi.set_variant('cuda_ad_rgb', 'llvm_ad_rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3f07f-e01a-4c36-bee3-eedc75575170",
   "metadata": {},
   "source": [
    "For convenience, we define a helper function to plot a list of images in one row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07868c5-3ad5-431b-aac4-be9075a42b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_list(images, title=None):\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(18, 3))\n",
    "    for i in range(len(images)):\n",
    "        axs[i].imshow(mi.util.convert_to_bitmap(images[i]))\n",
    "        axs[i].axis('off')\n",
    "    if title is not None:\n",
    "        plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdcd80",
   "metadata": {},
   "source": [
    "Helper function to plot a grid of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630321d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(images, rows, cols, title=None):\n",
    "    for i in range(rows):\n",
    "        plot_list(images[i*cols:(i+1)*cols], title+' (%d)' %i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15776c-ad95-41e2-b6d2-441e22720226",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "We now define a few parameters for the optimization pipeline implemented below. The optimization will start at a low resolution and then upsample the optimized volume parameters every `num_iterations_per_stage` iterations. This will be done a total of `num_stages` times. This coarse-to-fine scheme improves convergence and is a common heuristic used in differentiable rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2a277-6525-4c4a-8637-118be88ed308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering resolution\n",
    "render_res = 256\n",
    "\n",
    "# Number of stages\n",
    "num_stages = 4\n",
    "\n",
    "# Number of training iteration per stage\n",
    "num_iterations_per_stage = 15\n",
    "\n",
    "# learning rate \n",
    "learning_rate = 0.2\n",
    "\n",
    "# Initial grid resolution\n",
    "grid_init_res = 16\n",
    "\n",
    "# Spherical harmonic degree to be use for view-dependent appearance modeling\n",
    "sh_degree = 2\n",
    "\n",
    "# Enable ReLU in integrator\n",
    "use_relu = True\n",
    "\n",
    "# Number of sensors\n",
    "# Cameras will be distributed on a sphere around the center of the scene\n",
    "num_cols = 7\n",
    "num_rows = 1\n",
    "\n",
    "sensor_count = num_rows * num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ba5d1-c213-4c06-9ae4-aade1e21958a",
   "metadata": {},
   "source": [
    "## Creating multiple sensors\n",
    "\n",
    "As done in many of the other tutorials, we instantiate a couple of sensors to render our synthetic scene from different viewpoints. Here the cameras are placed in a circle around the `[0.5, 0.5, 0.5]` point which is going to be the center of our voxel grids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8e898",
   "metadata": {},
   "source": [
    "Define a helper class to more easily enable interactive movement of the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfe90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera:\n",
    "    #defaults\n",
    "    eyePoint = [0,0,0]\n",
    "    aimPoint = [0,0,1]\n",
    "    upVector = [0,1,0]\n",
    "    fov = 45\n",
    "\n",
    "    latitude = 0\n",
    "    longitude = 0\n",
    "\n",
    "    radius = 1\n",
    "\n",
    "    def __init__(self, eyePoint = [0,0,0], aimPoint = [0,0,1], upVector = [0,1,0], fov = 45):\n",
    "        self.eyePoint = eyePoint\n",
    "        self.aimPoint = aimPoint\n",
    "        self.upVector = upVector\n",
    "        self.fov = fov\n",
    "\n",
    "    #define the position of camera relative to aimPoint in spherical coordinates\n",
    "    def placeCamera(self, latitude, longitude, radius):\n",
    "        self.latitude = latitude\n",
    "        self.longitude = longitude\n",
    "        self.radius = radius\n",
    "\n",
    "        self.eyePoint[0] = self.aimPoint[0]+radius*math.sin(longitude * math.pi/180)*math.cos(latitude * math.pi/180)\n",
    "        self.eyePoint[1] = self.aimPoint[1]+radius*math.sin(latitude * math.pi/180)\n",
    "        self.eyePoint[2] = self.aimPoint[2]+radius*math.cos(longitude * math.pi/180)*math.cos(latitude * math.pi/180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b85aa8",
   "metadata": {},
   "source": [
    "Helper function to load a Mitsuba sensor with the parameters of a Camera object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77154fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSensor(camera):\n",
    "    return mi.load_dict({\n",
    "        'type': 'perspective',\n",
    "        'fov': camera.fov,\n",
    "        'to_world': mi.ScalarTransform4f.look_at(target = camera.aimPoint, origin = camera.eyePoint, up = camera.upVector),\n",
    "        'film': {\n",
    "            'type': 'hdrfilm',\n",
    "            'width': render_res,\n",
    "            'height': render_res,\n",
    "            'filter': {'type': 'box'},\n",
    "            'pixel_format': 'rgba'\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10333b5e-eb4a-4b97-bb97-7895f2d8cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_ref = mi.load_file('scenes/lego/scene.xml')\n",
    "\n",
    "sensors = []\n",
    "cameras = []\n",
    "def placeMultipleCameras(rows, cols):\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            longitude = 360.0 / cols * j\n",
    "            latitude = 90/rows * i\n",
    "\n",
    "            camera = Camera([0,0,0], [.5,.5,.5], [0,1,0], 45)\n",
    "\n",
    "            camera.placeCamera(latitude, longitude, 1.3)\n",
    "\n",
    "            sensor = loadSensor(camera)\n",
    "            cameras.append(camera)\n",
    "            sensors.append(sensor)\n",
    "\n",
    "placeMultipleCameras(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a16933",
   "metadata": {},
   "source": [
    "This helper function creates a GUI to edit any of the cameras in the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e20908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_render(index, scene, latitude, longitude, radius, fov):\n",
    "    camera = Camera([0,0,0], [.5,.5,.5], [0,1,0], fov)\n",
    "    camera.placeCamera(latitude,longitude,radius)\n",
    "\n",
    "    sensor = loadSensor(camera)\n",
    "\n",
    "    cameras[index] = camera\n",
    "    sensors[index] = loadSensor(camera)\n",
    "\n",
    "    image = mi.render(scene, sensor=sensors[index], spp=64)\n",
    "    tone_mapped = image.numpy() ** (.5)\n",
    "    tone_mapped[tone_mapped > 1] = 1\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(tone_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19877475",
   "metadata": {},
   "source": [
    "Use the index dropdown to select a camera to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2793461",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_index = 0\n",
    "\n",
    "def select_camera(index, scene):\n",
    "    cam_index = index\n",
    "    interact(interactive_render,\n",
    "        index = fixed(cam_index),\n",
    "        scene = fixed(scene),\n",
    "        latitude = widgets.FloatSlider(min = -89, max = 89, value = cameras[cam_index].latitude),\n",
    "        longitude = widgets.FloatSlider(min = 0, max = 360, value = cameras[cam_index].longitude),\n",
    "        radius = widgets.FloatSlider(min = 0, max = 10, value = cameras[cam_index].radius),\n",
    "        fov = widgets.FloatSlider(min = 0, max = 90, value = cameras[cam_index].fov)\n",
    ")\n",
    "\n",
    "interact(select_camera, index = widgets.Dropdown(options = range(sensor_count)), scene = fixed(scene_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483955f7-82a3-48ee-9600-3f553eb1020a",
   "metadata": {},
   "source": [
    "Run this cell to show all views at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2244-3224-4da5-a453-328920b8c550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_images = [mi.render(scene_ref, sensor=sensors[i], spp=64) for i in range(sensor_count)]\n",
    "\n",
    "plot_grid(ref_images, num_rows, num_cols, \"References images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8170770-8e54-4a03-9ed4-b36971ecf96e",
   "metadata": {},
   "source": [
    "## NeRF-like PRB integrator\n",
    "\n",
    "Unlike the other tutorials, this pipeline doesn't use any of the conventional physically-based rendering algorithms such as path-tracing. Instead, it implements a differentiable integrator for emissive volumes that directly uses a density and SH coefficient grid.\n",
    "\n",
    "We define an integrator `RadianceFieldPRB` that inherits from `mi.ad.common.RBIntegrator`. The `RBIntegrator` class is meant to be used to implement differentiable integrators \n",
    "that rely on tracing light paths to estimate derivatives (in contrast to purely using automatic differentiation). In the Mitsuba code base, this integrator base class is primarily used to implement various versions of [path replay backpropagation][1]. \n",
    "\n",
    "In the following implementation, we use this interface to implement a differentiable rendering method for purely emissive volumes (we don't consider scattering or indirect effects). We will use the path replay algorithm to implement a ray marching routine that does not need to allocate any large temporary buffers to differentiate the rendering process. This means that the memory use of this implementation will only depend on the size of the parameter grids, and not on the number of rays being evaluated at once.\n",
    "\n",
    "We implement the functions `__init__`, `eval_emission` `sample`, `traverse` and `parameters_changed`.\n",
    "\n",
    "In `__init__` we initialize a bounding box for our volume as well as 3D textures storing the density (`sigmat`) and SH coefficients (`sh_coeffs`). The number of channels for the SH coefficients depends on the chosen `sh_degree` (default: `2`). \n",
    "\n",
    "The `traverse` function simply returns the differentiable parameters and `parameters_changed` updates the 3D textures in case the differentiable parameters were updated (e.g., by a gradient step). The update of the 3D textures is necessary for Mitsuba variants where hardware-accelerated texture interpolation is used (i.e., `cuda_ad_rgb`). By invoking `texture.set_tensor(texture.tensor())`, we force Dr.Jit to update the underlying hardware texture.\n",
    "\n",
    "The main ray marching implementation is inside the `sample` function. This function returns the radiance along a single input ray. We first check if the ray intersects the volume's bounding box. We then use an `mi.Loop` to perform the ray marching routine. Inside the loop, we evaluate the density and spherical harmonics coefficients at the current point `p`. We accumulate radiance in a variable `L` and use `β` to store the current throughput. The directionally varying emission is evaluated using the `eval_emission` helper function. \n",
    "\n",
    "The `sample` function is written such that it can both be used for the primal computation (`primal = True`) and to accumulate gradients in reverse mode (`primal = False`). \n",
    "\n",
    "When computing the parameter gradients, we pass the gradient of the objective function (`δL`) and the output of the primal computation (`state_in`). Instead of accumulating radiance in the variable `L`, we initialize `L` from the primal output and then iteratively *subtract* emitted radiance to reconstruct gradient values. The `dr.backward_from` call backpropagates gradients all the way to the two 3D textures.\n",
    "\n",
    "In reverse mode, a significant part of the computation inside the loop is evaluated with gradients enabled. The `dr.resume_grad` scope enables gradients selectively inside the `with` block. The algorithm is designed to not build any AD graph across loop iterations.\n",
    "\n",
    "For a more detailed explanation of path replay, please see the [paper][1].\n",
    "\n",
    "\n",
    "[1]: http://rgl.epfl.ch/publications/Vicini2021PathReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82313a97-a314-43a3-8a4b-f4e32305f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadianceFieldPRB(mi.ad.common.RBIntegrator):\n",
    "    def __init__(self, props=mi.Properties()):\n",
    "        super().__init__(props)\n",
    "        self.bbox = mi.ScalarBoundingBox3f([0.0, 0.0, 0.0], [1.0, 1.0, 1.0])\n",
    "        self.use_relu = use_relu\n",
    "        self.grid_res = grid_init_res\n",
    "        # Initialize the 3D texture for the density and SH coefficients\n",
    "        res = self.grid_res\n",
    "        self.sigmat = mi.Texture3f(dr.full(mi.TensorXf, 0.01, shape=(res, res, res, 1)))\n",
    "        self.sh_coeffs = mi.Texture3f(dr.full(mi.TensorXf, 0.1, shape=(res, res, res, 3 * (sh_degree + 1) ** 2)))\n",
    "\n",
    "    def eval_emission(self, pos, direction): \n",
    "        spec = mi.Spectrum(0)\n",
    "        sh_dir_coef = dr.sh_eval(direction, sh_degree)\n",
    "        sh_coeffs = self.sh_coeffs.eval(pos)\n",
    "        for i, sh in enumerate(sh_dir_coef):\n",
    "            spec += sh * mi.Spectrum(sh_coeffs[3 * i:3 * (i + 1)])\n",
    "        return dr.clip(spec, 0.0, 1.0)\n",
    "\n",
    "    def sample(self, mode, scene, sampler,\n",
    "               ray, δL, state_in, active, **kwargs):\n",
    "        primal = mode == dr.ADMode.Primal\n",
    "        \n",
    "        ray = mi.Ray3f(ray)\n",
    "        hit, mint, maxt = self.bbox.ray_intersect(ray)\n",
    "        \n",
    "        active = mi.Bool(active)\n",
    "        active &= hit  # ignore rays that miss the bbox\n",
    "        if not primal:  # if the gradient is zero, stop early\n",
    "            active &= dr.any(dr.neq(δL, 0))\n",
    "\n",
    "        step_size = mi.Float(1.0 / self.grid_res)\n",
    "        t = mi.Float(mint) + sampler.next_1d(active) * step_size\n",
    "        L = mi.Spectrum(0.0 if primal else state_in)\n",
    "        δL = mi.Spectrum(δL if δL is not None else 0)\n",
    "        β = mi.Spectrum(1.0) # throughput\n",
    "        \n",
    "        loop = mi.Loop(name=f\"PRB ({mode.name})\",\n",
    "                       state=lambda: (sampler, ray, L, t, δL, β, active))\n",
    "        while loop(active):\n",
    "            p = ray(t)\n",
    "            with dr.resume_grad(when=not primal):\n",
    "                sigmat = self.sigmat.eval(p)[0]\n",
    "                if self.use_relu:\n",
    "                    sigmat = dr.maximum(sigmat, 0.0)\n",
    "                tr = dr.exp(-sigmat * step_size)\n",
    "                # Evaluate the directionally varying emission (weighted by transmittance)\n",
    "                Le = β * (1.0 - tr) * self.eval_emission(p, ray.d) \n",
    "                if not primal:\n",
    "                    dr.backward_from(δL * (L * tr / dr.detach(tr) + Le))\n",
    "            β *= tr\n",
    "            L = L + Le if primal else L - Le\n",
    "            t += step_size\n",
    "            active &= (t < maxt) & dr.any(dr.neq(β, 0.0))\n",
    "\n",
    "        return L if primal else δL, mi.Bool(True), L\n",
    "\n",
    "    def traverse(self, callback):\n",
    "        callback.put_parameter(\"sigmat\", self.sigmat.tensor(), mi.ParamFlags.Differentiable)\n",
    "        callback.put_parameter('sh_coeffs', self.sh_coeffs.tensor(), mi.ParamFlags.Differentiable)\n",
    "\n",
    "    def parameters_changed(self, keys):\n",
    "        self.sigmat.set_tensor(self.sigmat.tensor())\n",
    "        self.sh_coeffs.set_tensor(self.sh_coeffs.tensor())\n",
    "        self.grid_res = self.sigmat.shape[0]\n",
    "\n",
    "mi.register_integrator(\"rf_prb\", lambda props: RadianceFieldPRB(props))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10e22a-ad8c-4431-903a-22ea73bd62d2",
   "metadata": {},
   "source": [
    "## Setting up the optimization scene\n",
    "\n",
    "Here we set up our simple optimization scene. It is only composed of a constant area light and a `RadianceFieldPRB` integrator. No geometry or volume instance is needed since the integrator itself already contains the feature voxel grid.\n",
    "\n",
    "As shown in the rendered initial state, the scene appears empty at first as the density grid was initialized with a very low density value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9985c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = mi.load_dict({\n",
    "    'type': 'scene', \n",
    "    'integrator': {\n",
    "        'type': 'rf_prb'\n",
    "    }, \n",
    "    'emitter': {\n",
    "        'type': 'constant'\n",
    "    }\n",
    "})\n",
    "integrator = scene.integrator()\n",
    "\n",
    "# Render initial state\n",
    "init_images = [mi.render(scene, sensor=sensors[i], spp=128) for i in range(sensor_count)]\n",
    "\n",
    "plot_grid(init_images, num_rows, num_cols, 'Initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cf830-9ad3-4bd7-98ca-3840e3179378",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We use an `Adam` optimizer in this pipeline. The constructor of the optimizer takes the learning rate as well as a dictionary of optimized variables. In this tutorial, we want to optimize the density and spherical harmonics coefficients grids. We call `params.update(opt)` to ensure that the integrator is notified that some parameters now have gradients enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4e964-c5f9-4d7d-81d3-e8669bf153c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = mi.traverse(integrator)\n",
    "opt = mi.ad.Adam(lr=learning_rate, params={'sigmat': params['sigmat'], 'sh_coeffs': params['sh_coeffs']})\n",
    "params.update(opt);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f599926-83f5-42b9-a9fd-f01a37fc7c7d",
   "metadata": {},
   "source": [
    "Finally comes the main optimization loop of the pipeline. As in previous tutorials, at every iteration we render the scene from the different viewpoints and back-propagate the gradients through an L1 loss.\n",
    "\n",
    "For convenience we store intermediate renderings at the end of every stage to further inspect the optimization progress.\n",
    "\n",
    "Moreover, as stated earlier in this tutorial, we up-sample the feature voxel grids by a factor of two at the end of every stage. This can easily be achieved using the [<code>dr.upsample()</code>][1] routine.\n",
    "\n",
    "[1]: https://drjit.readthedocs.io/en/latest/src/api_reference.html#upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af17b50-c809-472e-a55d-4b1589a5e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "intermediate_images = []\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"Stage {stage+1:02d}, feature voxel grids resolution -> {opt['sigmat'].shape[0]}\")\n",
    "    \n",
    "    for it in range(num_iterations_per_stage):\n",
    "        total_loss = 0.0\n",
    "        images = []\n",
    "        for sensor_idx in range(sensor_count):\n",
    "            img = mi.render(scene, params, sensor=sensors[sensor_idx], spp=1, seed=it)\n",
    "            loss = dr.mean(dr.abs(img - ref_images[sensor_idx]))\n",
    "            dr.backward(loss)\n",
    "            total_loss += loss[0]\n",
    "            \n",
    "            # Store images at the end of every stage\n",
    "            if it == num_iterations_per_stage - 1:\n",
    "                dr.eval(img)\n",
    "                images.append(img)\n",
    "            \n",
    "        losses.append(total_loss)\n",
    "        opt.step()\n",
    "\n",
    "        if not integrator.use_relu:\n",
    "            opt['sigmat'] = dr.maximum(opt['sigmat'], 0.0)\n",
    "            \n",
    "        params.update(opt)\n",
    "        print(f\"  --> iteration {it+1:02d}: error={total_loss:6f}\", end='\\r')\n",
    "\n",
    "    intermediate_images.append(images)\n",
    "    \n",
    "    # Upsample the 3D textures at every stage\n",
    "    if stage < num_stages - 1: \n",
    "        new_res = 2 * opt['sigmat'].shape[0]\n",
    "        new_shape = [new_res, new_res, new_res]\n",
    "        opt['sigmat']   = dr.upsample(opt['sigmat'],   new_shape)\n",
    "        opt['sh_coeffs'] = dr.upsample(opt['sh_coeffs'], new_shape)\n",
    "        params.update(opt)\n",
    "\n",
    "print('')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc0be6-04a7-490f-bcc8-105e54f8097c",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We render the final images at higher SPP and display the results at every stages as well as the reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3bf2f-21f7-4635-8fce-261a70cd5062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_images = [mi.render(scene, sensor=sensors[i], spp=128) for i in range(sensor_count)]\n",
    "for stage, inter in enumerate(intermediate_images):\n",
    "    plot_grid(inter, num_rows, num_cols, f'Stage {stage}')\n",
    "\n",
    "plot_grid(final_images, num_rows, num_cols, 'Final')\n",
    "\n",
    "plot_grid(ref_images, num_rows, num_cols, 'Reference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc44505-3ba4-4208-9d6b-c2227dc39e7d",
   "metadata": {},
   "source": [
    "We can also take a closer look at one of the view point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033ead8-a65e-484b-bd72-2da9993b9099",
   "metadata": {
    "nbsphinx-thumbnail": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].imshow(mi.util.convert_to_bitmap(final_images[1]))\n",
    "axs[0].set_title('Reconstructed')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(mi.util.convert_to_bitmap(ref_images[1]))\n",
    "axs[1].set_title('Reference')\n",
    "axs[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f804ff-2e5d-46ca-854e-e2566eb59b33",
   "metadata": {},
   "source": [
    "When working with optimization pipelines, it is always very informative to take a look at the graph of objective function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e5d64-0ad7-44a7-8574-04e93e8f1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe70c6f",
   "metadata": {},
   "source": [
    "[[-1, 0, 0, 0.5],\n",
    " [0, 1, 0, 0.5],\n",
    " [0, 0, -1, 1.8],\n",
    " [0, 0, 0, 1]]\n",
    "[[-0.62349, 0, -0.781831, 1.51638],\n",
    " [0, 1, 0, 0.5],\n",
    " [0.781831, 0, -0.62349, 1.31054],\n",
    " [0, 0, 0, 1]]\n",
    "[[0.222521, 0, -0.974928, 1.76741],\n",
    " [0, 1, 0, 0.5],\n",
    " [0.974928, 0, 0.222521, 0.210723],\n",
    " [0, 0, 0, 1]]\n",
    "[[0.900969, 0, -0.433884, 1.06405],\n",
    " [0, 1, 0, 0.5],\n",
    " [0.433884, 0, 0.900969, -0.67126],\n",
    " [0, 0, 0, 1]]\n",
    "[[0.900969, 0, 0.433884, -0.0640486],\n",
    " [0, 1, 0, 0.5],\n",
    " [-0.433884, 0, 0.900969, -0.67126],\n",
    " [0, 0, 0, 1]]\n",
    "[[0.222521, 0, 0.974928, -0.767406],\n",
    " [0, 1, 0, 0.5],\n",
    " [-0.974928, 0, 0.222521, 0.210723],\n",
    " [0, 0, 0, 1]]\n",
    "[[-0.62349, 0, 0.781831, -0.516381],\n",
    " [0, 1, 0, 0.5],\n",
    " [-0.781831, 0, -0.62349, 1.31054],\n",
    " [0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce4e36",
   "metadata": {},
   "source": [
    "Interactively view the final radiance field. Note the severe reconstruction errors in interpolated views. A much larger number of input views (100 is typical for 360 degree camera movement) is required for higher reconstruction quality. Try editing the training parameters above and running this notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d927ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(select_camera, index = widgets.Dropdown(options = range(sensor_count)), scene = fixed(scene))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec4df2",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "- [<code>mitsuba.ad.integrators.RBIntegrator</code>](https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.ad.integrators.RBIntegrator)\n",
    "- [<code>mitsuba.TensorXf</code>](https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.TensorXf)\n",
    "- [<code>mitsuba.Texture3f</code>](https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.Texture3f)\n",
    "- [<code>dr.sh_eval</code>](https://drjit.readthedocs.io/en/latest/src/api_reference.html#sh_eval)\n",
    "- [<code>dr.upsample</code>](https://drjit.readthedocs.io/en/latest/src/api_reference.html#upsample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f1906eb39b94b37c0d58a5b9509c6228216c0671bf58e53d61e1e211985e40d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
